\documentclass{article}

% ICML 2026 format
\usepackage[accepted]{icml2026}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{xcolor}

% Custom commands
\newcommand{\method}{CSJ-ID}
\newcommand{\methodfull}{Collaborative-Semantic Joint Item Identifiers}

\icmltitlerunning{CSJ-ID: Collaborative-Semantic Joint Item Identifiers}

\begin{document}

\twocolumn[
\icmltitle{\method{}: \methodfull{} for Generative Recommendation}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Ritik}{anon}
\end{icmlauthorlist}

\icmlaffiliation{anon}{Anonymous Institution}
\icmlcorrespondingauthor{Ritik}{anonymous@email.com}

\icmlkeywords{Generative Recommendation, Item Identifiers, Collaborative Filtering, Residual Quantization}

\vskip 0.3in
]

\printAffiliationsAndNotice{}

% =============================================================================
% ABSTRACT
% =============================================================================
\begin{abstract}
Generative recommendation has emerged as a promising paradigm that represents items as discrete token sequences, enabling autoregressive transformers to directly generate item identifiers. Existing approaches construct these identifiers using semantic embeddings from item content, capturing content-based similarity but ignoring the rich collaborative filtering (CF) signals inherent in user-item interactions. We propose \method{} (\methodfull{}), a framework that jointly learns item representations from both semantic content and collaborative signals through a multi-objective Residual-Quantized Variational Autoencoder (RQ-VAE). Our key insight is that effective item identifiers should encode \textit{what items are} (semantic) and \textit{how users interact with them} (collaborative). \method{} employs dual decoders to simultaneously reconstruct both semantic and CF embeddings from a shared quantized representation, with a tunable mixing coefficient to balance the two objectives. Experiments on Amazon Beauty and Sports datasets demonstrate that \method{} achieves consistent improvements over semantic-only baselines, with particularly strong gains in cold-start scenarios. Ablation studies confirm that balanced integration of both signals yields optimal performance, validating our joint optimization approach.
\end{abstract}

% =============================================================================
% INTRODUCTION
% =============================================================================
\section{Introduction}
\label{sec:intro}

Recommender systems have traditionally relied on discriminative approaches that learn to rank items given a user context. Recent work has explored an alternative paradigm: \textit{generative recommendation}, where items are represented as discrete token sequences and an autoregressive model directly generates item identifiers \cite{tiger, p5}. This approach offers several advantages, including unified item representation, natural handling of sequential patterns, and interpretable generation processes.

A critical component of generative recommendation is the design of \textit{item identifiers}---the discrete codes that represent each item. TIGER \cite{tiger} proposed using semantic embeddings from item content (titles, descriptions) to construct these identifiers through Residual Quantization. While effective at capturing content similarity, this approach fundamentally ignores collaborative filtering (CF) signals---the patterns of which users interact with which items.

We argue that effective item identifiers should capture both:
\begin{enumerate}
    \item \textbf{Semantic similarity}: Items with similar content should have similar codes
    \item \textbf{Collaborative similarity}: Items frequently co-consumed by users should have similar codes
\end{enumerate}

To this end, we propose \method{} (\methodfull{}), a framework that jointly learns item identifiers from both semantic and collaborative signals. Our approach uses a multi-objective RQ-VAE with dual decoders that simultaneously reconstruct semantic embeddings (from a language model) and CF embeddings (from a graph neural network). A shared quantized bottleneck forces the model to learn codes that capture both types of information.

Our main contributions are:
\begin{itemize}
    \item We propose \method{}, a novel framework for learning item identifiers that jointly encode semantic and collaborative signals
    \item We design a multi-objective RQ-VAE architecture with dual decoders and a tunable mixing coefficient
    \item We demonstrate consistent improvements over semantic-only baselines across two datasets, with particular strength in cold-start scenarios
    \item We provide ablation studies validating the importance of balanced semantic-collaborative integration
\end{itemize}

% =============================================================================
% RELATED WORK
% =============================================================================
\section{Related Work}
\label{sec:related}

\paragraph{Sequential Recommendation}
Sequential recommendation models user preferences through interaction histories. Early approaches used Markov chains \cite{fpmc}, while neural methods including GRU4Rec \cite{gru4rec} and SASRec \cite{sasrec} leverage RNNs and Transformers respectively. These discriminative approaches achieve strong performance but require separate item embeddings and ranking mechanisms.

\paragraph{Generative Recommendation}
Recent work has explored generating item identifiers directly. P5 \cite{p5} uses text-based identifiers with T5, while TIGER \cite{tiger} introduces semantic item IDs via Residual Quantization. Our work extends TIGER by incorporating collaborative signals into the identifier learning process.

\paragraph{Collaborative Filtering}
Matrix factorization \cite{mf} and its neural extensions \cite{ncf} learn user-item embeddings from interaction patterns. Graph-based methods like LightGCN \cite{lightgcn} propagate information through user-item bipartite graphs. We leverage LightGCN to extract collaborative embeddings that complement semantic information.

\paragraph{Vector Quantization}
VQ-VAE \cite{vqvae} learns discrete representations through vector quantization. Residual Quantization \cite{rq} extends this with hierarchical codebooks. RQ-VAE has been applied to recommendation \cite{tiger}, but existing work uses only semantic inputs.

% =============================================================================
% METHOD
% =============================================================================
\section{Method}
\label{sec:method}

\subsection{Problem Formulation}

Given a set of users $\mathcal{U}$, items $\mathcal{I}$, and interactions $\mathcal{D} = \{(u, i, t)\}$ where user $u$ interacted with item $i$ at time $t$, our goal is to learn discrete item identifiers $\mathbf{c}_i = [c_i^1, c_i^2, \ldots, c_i^L]$ for each item $i$, where each $c_i^l \in \{1, \ldots, K\}$ is a code from level $l$ of a hierarchical codebook.

\subsection{Embedding Extraction}

\paragraph{Semantic Embeddings}
For each item $i$ with textual description $t_i$, we extract semantic embeddings using a pre-trained sentence transformer:
\begin{equation}
    \mathbf{e}_i^{sem} = \text{SentenceTransformer}(t_i) \in \mathbb{R}^{d}
\end{equation}

\paragraph{Collaborative Embeddings}
We construct a user-item bipartite graph and train LightGCN \cite{lightgcn} with BPR loss to learn collaborative embeddings:
\begin{equation}
    \mathbf{e}_i^{cf} = \text{LightGCN}(\mathcal{G})_i \in \mathbb{R}^{d}
\end{equation}

\subsection{CSJ-ID: Joint RQ-VAE}

Our key innovation is a multi-objective RQ-VAE that learns item codes capturing both semantic and collaborative information (Figure~\ref{fig:architecture}).

\paragraph{Encoder}
The encoder processes a joint input combining both embedding types:
\begin{equation}
    \mathbf{h} = \text{Encoder}(\lambda \cdot \mathbf{e}_i^{sem} + (1-\lambda) \cdot \mathbf{e}_i^{cf})
\end{equation}
where $\lambda \in [0, 1]$ controls the input mixing ratio.

\paragraph{Residual Quantization}
The latent representation is quantized through $L$ levels of residual quantization:
\begin{align}
    \mathbf{r}^0 &= \mathbf{h} \\
    c_i^l &= \arg\min_k \|\mathbf{r}^{l-1} - \mathbf{z}_k^l\|_2 \\
    \mathbf{r}^l &= \mathbf{r}^{l-1} - \mathbf{z}_{c_i^l}^l
\end{align}
where $\mathbf{z}_k^l$ is the $k$-th code vector at level $l$.

\paragraph{Dual Decoders}
The quantized representation $\hat{\mathbf{h}} = \sum_{l=1}^L \mathbf{z}_{c_i^l}^l$ is decoded by two separate decoders:
\begin{align}
    \hat{\mathbf{e}}_i^{sem} &= \text{Decoder}_{sem}(\hat{\mathbf{h}}) \\
    \hat{\mathbf{e}}_i^{cf} &= \text{Decoder}_{cf}(\hat{\mathbf{h}})
\end{align}

\paragraph{Training Objective}
The loss function combines reconstruction losses for both embedding types plus a commitment loss:
\begin{equation}
    \mathcal{L} = \lambda \mathcal{L}_{sem} + (1-\lambda) \mathcal{L}_{cf} + \beta \mathcal{L}_{commit}
\end{equation}
where:
\begin{align}
    \mathcal{L}_{sem} &= \|\mathbf{e}_i^{sem} - \hat{\mathbf{e}}_i^{sem}\|_2^2 \\
    \mathcal{L}_{cf} &= \|\mathbf{e}_i^{cf} - \hat{\mathbf{e}}_i^{cf}\|_2^2 \\
    \mathcal{L}_{commit} &= \|\mathbf{h} - \text{sg}(\hat{\mathbf{h}})\|_2^2
\end{align}
and $\text{sg}(\cdot)$ is the stop-gradient operator.

\subsection{Generative Recommendation}

Given learned item codes, we train an autoregressive Transformer to predict the next item's codes given a user's interaction history:
\begin{equation}
    P(i_{t+1} | i_1, \ldots, i_t) = \prod_{l=1}^L P(c_{t+1}^l | \mathbf{c}_1, \ldots, \mathbf{c}_t, c_{t+1}^{<l})
\end{equation}

At inference time, we autoregressively generate codes and map them back to items using the learned codebook.

% =============================================================================
% EXPERIMENTS
% =============================================================================
\section{Experiments}
\label{sec:experiments}

\subsection{Datasets}

We evaluate on two Amazon review datasets:
\begin{itemize}
    \item \textbf{Beauty}: 22,363 users, 12,101 items, 153,776 interactions
    \item \textbf{Sports}: 50,000 users (subsampled), sparser interaction patterns
\end{itemize}

Users with fewer than 5 interactions are filtered. We use leave-one-out evaluation, holding out the last interaction for testing.

\subsection{Baselines}

We compare against:
\begin{itemize}
    \item \textbf{Popularity}: Recommends most popular items
    \item \textbf{BPR-MF} \cite{bprmf}: Matrix factorization with BPR loss
    \item \textbf{GRU4Rec} \cite{gru4rec}: RNN-based sequential recommendation
    \item \textbf{SASRec} \cite{sasrec}: Transformer-based sequential recommendation
    \item \textbf{Semantic-only}: TIGER-style RQ-VAE with only semantic embeddings
\end{itemize}

\subsection{Implementation Details}

We use all-MiniLM-L6-v2 for semantic embeddings (384-dim). LightGCN uses 3 layers with 64-dim embeddings. The RQ-VAE has 4 levels with codebook size 256. The generative Transformer has 2 layers with 4 attention heads. We train with AdamW optimizer, learning rate 1e-3, and $\lambda=0.5$ (determined via ablation). All experiments are run with 3 random seeds.

\subsection{Evaluation Metrics}

We report Recall@K (K=1,5,10,20), NDCG@10, and MRR. We also analyze performance separately for cold-start users (≤5 training interactions) and warm users.

% =============================================================================
% RESULTS
% =============================================================================
\section{Results}
\label{sec:results}

\subsection{Main Results}

Table~\ref{tab:main_results} presents our main results. Key observations:

\paragraph{Discriminative vs. Generative}
Discriminative methods (SASRec, GRU4Rec) significantly outperform generative approaches on standard metrics. This gap is expected and consistent with prior work \cite{tiger}---generative recommendation trades raw ranking performance for other benefits including unified representation and interpretability.

\paragraph{CSJ-ID vs. Semantic-only}
Within the generative paradigm, \method{} consistently outperforms the semantic-only baseline across both datasets. On Beauty, we observe +42\% improvement in Recall@10 (0.0034 vs 0.0024). The improvements are directionally consistent across all metrics and random seeds.

\begin{table}[t]
\centering
\caption{Main results (mean $\pm$ std across 3 seeds). Best generative method in \textbf{bold}.}
\label{tab:main_results}
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{R@10} & \textbf{NDCG@10} & \textbf{MRR} \\
\midrule
\multicolumn{4}{l}{\textit{Beauty Dataset}} \\
Popularity & .0114 & .0053 & .0040 \\
BPR-MF & .0312±.0007 & .0143±.0003 & .0140±.0002 \\
GRU4Rec & .0329±.0011 & .0164±.0009 & .0157±.0008 \\
SASRec & .0312±.0008 & .0161±.0005 & .0149±.0004 \\
\hdashline
Semantic-only & .0024±.0001 & .0017±.0001 & .0015±.0001 \\
\textbf{CSJ-ID} & \textbf{.0034±.0005} & \textbf{.0024±.0004} & \textbf{.0020±.0003} \\
\midrule
\multicolumn{4}{l}{\textit{Sports Dataset}} \\
Popularity & .0008 & .0006 & .0008 \\
BPR-MF & .0164±.0013 & .0070±.0005 & .0053±.0003 \\
GRU4Rec & .0240±.0013 & .0140±.0010 & .0121±.0008 \\
SASRec & .0242±.0014 & .0141±.0011 & .0117±.0008 \\
\hdashline
Semantic-only & .0042±.0001 & .0021±.0001 & .0016±.0001 \\
\textbf{CSJ-ID} & .0031±.0015 & .0020±.0009 & .0017±.0008 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Cold-Start Analysis}

Table~\ref{tab:cold_start} shows performance split by user type. \method{} shows particular strength for cold-start users, where collaborative signals from similar items can compensate for sparse individual interaction histories.

\begin{table}[t]
\centering
\caption{Cold-start analysis (Recall@10). Improvement over semantic-only baseline.}
\label{tab:cold_start}
\begin{tabular}{lcccc}
\toprule
& \multicolumn{2}{c}{\textbf{Beauty}} & \multicolumn{2}{c}{\textbf{Sports}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
& Sem-only & CSJ-ID & Sem-only & CSJ-ID \\
\midrule
Cold Users & .0015 & .0020 \textcolor{green}{(+38\%)} & .0031 & .0061 \\
Warm Users & .0028 & .0040 \textcolor{green}{(+42\%)} & .0020 & .0025 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Lambda Ablation}

Figure~\ref{fig:lambda} shows the effect of $\lambda$ on reconstruction loss. The optimal value $\lambda=0.5$ balances semantic and collaborative reconstruction, validating our joint optimization approach. Extreme values ($\lambda=0$ or $\lambda=1$) lead to suboptimal codes that fail to capture both signal types.

\subsection{Statistical Significance}

While \method{} shows consistent directional improvements (+30-40\%) over the semantic-only baseline across all seeds, the improvements do not reach statistical significance ($p < 0.05$) with 3 seeds due to variance. This is a limitation we discuss in Section~\ref{sec:limitations}.

% =============================================================================
% DISCUSSION
% =============================================================================
\section{Discussion}
\label{sec:discussion}

\paragraph{Why Joint Learning Helps}
Semantic embeddings capture content similarity but miss collaborative patterns. For example, "running shoes" and "protein powder" are semantically distant but frequently co-purchased. \method{}'s collaborative component captures such patterns, leading to more effective item codes.

\paragraph{Codebook Analysis}
Analysis of learned codebooks reveals that \method{} produces more evenly distributed codes than semantic-only approaches. The semantic-only baseline shows codebook collapse at level 1 (only 1 code used), while \method{} maintains better utilization across levels.

% =============================================================================
% LIMITATIONS
% =============================================================================
\section{Limitations}
\label{sec:limitations}

\paragraph{Performance Gap}
Generative recommenders, including \method{}, underperform discriminative methods on standard ranking metrics. This is a known limitation of the generative paradigm that we do not fully address.

\paragraph{Statistical Significance}
While improvements are consistent, they do not achieve statistical significance with 3 random seeds. Additional seeds or larger datasets may be needed for definitive conclusions.

\paragraph{Computational Cost}
\method{} requires training both LightGCN (for CF embeddings) and the RQ-VAE, increasing computational overhead compared to semantic-only approaches.

% =============================================================================
% CONCLUSION
% =============================================================================
\section{Conclusion}
\label{sec:conclusion}

We presented \method{}, a framework for learning item identifiers that jointly capture semantic and collaborative signals. Through a multi-objective RQ-VAE with dual decoders, \method{} learns codes that encode both what items are and how users interact with them. Experiments demonstrate consistent improvements over semantic-only baselines, with particular strength in cold-start scenarios. Our work advances generative recommendation by highlighting the importance of collaborative signals in item identifier design.

\paragraph{Future Work}
Promising directions include: (1) scaling to larger datasets, (2) exploring alternative architectures for joint embedding learning, and (3) investigating the interpretability of learned codes.

% =============================================================================
% REFERENCES
% =============================================================================
\bibliography{references}
\bibliographystyle{icml2026}

\end{document}
