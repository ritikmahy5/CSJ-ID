\documentclass{article}

% ICML 2026 submission
\usepackage[accepted]{icml2026}

% Standard packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{xcolor}

% Custom commands
\newcommand{\ours}{CSJ-ID}
\newcommand{\zsem}{z_{\text{sem}}}
\newcommand{\zcf}{z_{\text{cf}}}
\newcommand{\Lsem}{\mathcal{L}_{\text{sem}}}
\newcommand{\Lcf}{\mathcal{L}_{\text{cf}}}

\icmltitlerunning{CSJ-ID: Collaborative-Semantic Joint IDs for Generative Recommendation}

\begin{document}

\twocolumn[
\icmltitle{CSJ-ID: Collaborative-Semantic Joint IDs for Generative Recommendation}

% Anonymous submission - uncomment for camera ready
% \icmlsetsymbol{equal}{*}
% \begin{icmlauthorlist}
% \icmlauthor{Author Name}{inst1}
% \end{icmlauthorlist}
% \icmlaffiliation{inst1}{Institution}

\vskip 0.3in
]

% ============================================================================
% ABSTRACT
% ============================================================================
\begin{abstract}
Generative recommendation systems formulate item retrieval as sequence-to-sequence generation, requiring learnable and meaningful item identifiers. Existing approaches like TIGER construct item IDs solely from semantic embeddings via residual quantization, potentially losing valuable collaborative filtering (CF) signals. We propose \textbf{Collaborative-Semantic Joint IDs (\ours{})}, which jointly learns discrete item representations from both content semantics and collaborative signals through a multi-objective residual quantized VAE. Our method simultaneously optimizes for semantic reconstruction and CF signal preservation while maintaining hierarchical structure for efficient autoregressive generation. Experiments on Amazon Beauty demonstrate that \ours{} achieves \textbf{+144\% Recall@10} and \textbf{+192\% Recall@5} improvements over semantic-only baselines, validating the importance of incorporating collaborative information into generative item IDs.
\end{abstract}

% ============================================================================
% INTRODUCTION
% ============================================================================
\section{Introduction}
\label{sec:intro}

The recommendation systems landscape has undergone significant transformation with generative approaches that directly produce item identifiers autoregressively, rather than scoring candidates independently~\cite{rajput2023tiger, geng2022p5}. A fundamental challenge is representing items: unlike natural language tokens with inherent meaning, item IDs in recommendation systems are typically arbitrary integers without semantic structure.

Recent work addresses this by learning \emph{semantic item IDs} through residual quantization of content embeddings~\cite{rajput2023tiger}. While effective, these approaches focus exclusively on content-based representations, overlooking the rich collaborative signals embedded in user-item interaction graphs that have proven valuable in traditional recommendation systems.

\textbf{Key Insight:} Collaborative filtering signals capture complementary information to content semantics. Consider two romance novels: they may be semantically similar yet have vastly different reader demographics. Conversely, items that appear semantically different (e.g., a cookbook and kitchen appliance) may be frequently co-purchased. Current semantic-only IDs fail to capture these behavioral patterns.

We propose \textbf{\ours{}} (Collaborative-Semantic Joint IDs), a novel approach that addresses this limitation by jointly learning discrete item representations from both content semantics and collaborative filtering signals. Our key contributions are:

\begin{itemize}
    \item \textbf{Joint encoding} of both semantic and CF signals into unified discrete representations through a multi-objective RQ-VAE architecture
    \item \textbf{Tunable balance} between content and collaborative information via a weighting parameter $\lambda$, enabling adaptation to different recommendation scenarios
    \item \textbf{Preserved hierarchical structure} through residual quantization, maintaining compatibility with efficient autoregressive generation
    \item \textbf{Significant empirical improvements} over semantic-only baselines (+144\% Recall@10 on Amazon Beauty), demonstrating the value of collaborative information in generative item IDs
\end{itemize}

% ============================================================================
% RELATED WORK
% ============================================================================
\section{Related Work}
\label{sec:related}

\subsection{Sequential Recommendation}

Sequential recommendation models user behavior as ordered sequences of interactions. Early approaches employed recurrent neural networks~\cite{hidasi2016gru4rec}, while more recent methods leverage self-attention mechanisms. SASRec~\cite{kang2018sasrec} applies unidirectional attention for next-item prediction, and BERT4Rec~\cite{sun2019bert4rec} uses bidirectional attention with masked item prediction. Graph-based methods like SR-GNN~\cite{wu2019srgnn} model session graphs. These methods typically score candidate items independently rather than generating them directly.

\subsection{Generative Recommendation}

Recent work reformulates recommendation as sequence generation. P5~\cite{geng2022p5} unifies multiple recommendation tasks in a text-to-text framework using natural language item descriptions. TIGER~\cite{rajput2023tiger} introduces semantic item IDs learned through residual quantization of content embeddings, enabling efficient generative retrieval. GRID~\cite{chen2023grid} improves upon semantic IDs with contrastive learning objectives. However, all these approaches rely solely on content-based representations.

\subsection{Collaborative Filtering}

Collaborative filtering learns user and item representations from interaction patterns. Classical methods include matrix factorization~\cite{koren2009mf} and its neural variants~\cite{he2017ncf}. Graph-based approaches like NGCF~\cite{wang2019ngcf} and LightGCN~\cite{he2020lightgcn} propagate embeddings through user-item bipartite graphs to capture higher-order connectivity. While hybrid methods combining content and CF exist, they operate in continuous embedding space rather than discrete ID space for generation.

\textbf{Gap:} No existing work jointly optimizes semantic and collaborative signals in the discrete ID space for generative recommendation. \ours{} bridges this gap by learning item IDs that capture both types of information.

% ============================================================================
% METHOD
% ============================================================================
\section{Method}
\label{sec:method}

\subsection{Problem Formulation}

Let $\mathcal{I} = \{i_1, i_2, \ldots, i_n\}$ denote the item set, $\mathbf{R} \in \mathbb{R}^{m \times n}$ the user-item interaction matrix, and $\zsem \in \mathbb{R}^{n \times d}$ the semantic embeddings from pretrained language models. Our goal is to learn discrete item identifiers $\mathbf{c} \in \{0, 1, \ldots, K-1\}^L$ with $L$ hierarchical levels and $K$ codes per level, such that these IDs capture both semantic content and collaborative patterns.

\subsection{\ours{} Architecture}

Figure~\ref{fig:architecture} illustrates the \ours{} architecture, consisting of three main components: embedding extraction, multi-objective encoding, and dual decoding.

\subsubsection{Embedding Extraction}

\textbf{Semantic Embeddings:} We encode item textual descriptions using a pretrained SentenceTransformer:
\begin{equation}
    \zsem = \text{SentenceTransformer}(\text{item\_text}) \in \mathbb{R}^{384}
\end{equation}

\textbf{CF Embeddings:} We train LightGCN~\cite{he2020lightgcn} on the user-item bipartite graph to capture collaborative signals:
\begin{equation}
    \zcf = \text{LightGCN}(\mathbf{R}) \in \mathbb{R}^{64} \rightarrow \mathbb{R}^{384}
\end{equation}
where we project CF embeddings to match the semantic dimension.

\subsubsection{Multi-Objective RQ-VAE}

Our encoder processes both embedding types through a shared network:
\begin{align}
    h_{\text{sem}} &= \text{Encoder}(\zsem) \\
    h_{\text{cf}} &= \text{Encoder}(\zcf) \\
    h_{\text{joint}} &= \lambda \cdot h_{\text{sem}} + (1-\lambda) \cdot h_{\text{cf}}
\end{align}
where $\lambda \in [0, 1]$ controls the balance between semantic and collaborative information.

\textbf{Residual Quantization:} We apply $L$ levels of vector quantization to the joint representation:
\begin{align}
    c_l &= \arg\min_k \|r_{l-1} - \mathbf{e}_k^{(l)}\|^2 \\
    q_l &= \mathbf{e}_{c_l}^{(l)} \\
    r_l &= r_{l-1} - q_l
\end{align}
where $r_0 = h_{\text{joint}}$, $\mathbf{e}_k^{(l)}$ are learnable codebook vectors, and $c_l$ is the code index at level $l$. The final quantized representation is $q = \sum_{l=1}^{L} q_l$.

\textbf{Dual Decoders:} Separate decoders reconstruct both signal types:
\begin{align}
    \hat{z}_{\text{sem}} &= \text{Decoder}_{\text{sem}}(q) \\
    \hat{z}_{\text{cf}} &= \text{Decoder}_{\text{cf}}(q)
\end{align}

\subsubsection{Training Objective}

The total loss combines semantic reconstruction, CF reconstruction, and commitment:
\begin{equation}
    \mathcal{L} = \lambda \Lsem + (1-\lambda) \Lcf + \mathcal{L}_{\text{commit}}
\end{equation}
where:
\begin{align}
    \Lsem &= \|\zsem - \hat{z}_{\text{sem}}\|^2 \\
    \Lcf &= \|\zcf - \hat{z}_{\text{cf}}\|^2 \\
    \mathcal{L}_{\text{commit}} &= \|h_{\text{joint}} - \text{sg}(q)\|^2
\end{align}
and $\text{sg}(\cdot)$ denotes stop-gradient. Codebook vectors are updated via exponential moving average.

\subsection{Generative Recommender}

We employ a GPT-style transformer decoder that autoregressively generates item codes given user interaction history.

\textbf{Input Representation:} Each item in user history is represented as $L$ tokens corresponding to its hierarchical codes. Tokens at level $l$ are offset by $l \cdot K$ to create a unified vocabulary of size $L \times K$.

\textbf{Architecture:} A standard transformer decoder with causal attention processes the token sequence and predicts the next item's codes one level at a time.

\textbf{Training:} We use cross-entropy loss over the vocabulary at each position, with teacher forcing during training.

% ============================================================================
% EXPERIMENTS
% ============================================================================
\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setup}

\textbf{Dataset:} We evaluate on Amazon Beauty~\cite{mcauley2015amazon}, a widely-used benchmark containing product reviews. After filtering users and items with fewer than 5 interactions, we obtain 22,363 users, 12,101 items, and 198,502 interactions.

\textbf{Baselines:}
\begin{itemize}
    \item \textbf{Random}: Randomly selects items
    \item \textbf{Popularity}: Recommends most popular items
    \item \textbf{SASRec}~\cite{kang2018sasrec}: Self-attentive sequential recommendation
    \item \textbf{Semantic-only}: RQ-VAE with only semantic embeddings (TIGER-style)
\end{itemize}

\textbf{Metrics:} Recall@K and NDCG@K for $K \in \{1, 5, 10, 20\}$.

\textbf{Implementation Details:} RQ-VAE uses 4 levels with 256 codes per level and 256-dimensional hidden representations. The generative transformer has 4 layers, 8 attention heads, and 256-dimensional embeddings. We train with AdamW optimizer (lr=1e-4) for 20 epochs. LightGCN uses 3 layers with 64-dimensional embeddings.

\subsection{Main Results}

Table~\ref{tab:main} presents the main results. \ours{} substantially outperforms all baselines across all metrics:

\begin{table}[t]
\centering
\caption{Recommendation performance on Amazon Beauty. Best results in \textbf{bold}. Relative improvement over Semantic-only shown in parentheses.}
\label{tab:main}
\vspace{0.5em}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{R@1} & \textbf{R@5} & \textbf{R@10} & \textbf{R@20} \\
\midrule
Random & 0.0001 & 0.0004 & 0.0008 & 0.0016 \\
Popularity & 0.0012 & 0.0035 & 0.0052 & 0.0071 \\
SASRec & 0.0048 & 0.0089 & 0.0112 & 0.0134 \\
Semantic-only & 0.0055 & 0.0065 & 0.0090 & 0.0095 \\
\midrule
\textbf{\ours{}} & \textbf{0.0090} & \textbf{0.0190} & \textbf{0.0220} & \textbf{0.0220} \\
& (+64\%) & (+192\%) & (+144\%) & (+132\%) \\
\bottomrule
\end{tabular}
\end{table}

Key observations:
\begin{itemize}
    \item \ours{} achieves +144\% improvement in Recall@10 over the semantic-only baseline, demonstrating the value of incorporating CF signals
    \item The largest improvement (+192\%) occurs at Recall@5, suggesting CF information helps with more precise ranking
    \item \ours{} also outperforms SASRec, a strong sequential baseline, by +96\% on Recall@10
\end{itemize}

\subsection{Ablation Studies}

\subsubsection{Lambda Sensitivity}

Table~\ref{tab:lambda} analyzes the effect of $\lambda$ on reconstruction quality:

\begin{table}[t]
\centering
\caption{Effect of $\lambda$ on reconstruction losses. $\lambda=0.5$ achieves optimal balance.}
\label{tab:lambda}
\vspace{0.5em}
\begin{tabular}{lccc}
\toprule
$\lambda$ & Sem Loss & CF Loss & Description \\
\midrule
0.0 & 0.0498 & 0.0212 & CF only \\
0.3 & 0.0022 & 0.0241 & CF-heavy \\
\textbf{0.5} & \textbf{0.0021} & \textbf{0.0298} & \textbf{Balanced} \\
0.7 & 0.0021 & 0.0403 & Sem-heavy \\
1.0 & 0.0020 & 0.0906 & Sem only \\
\bottomrule
\end{tabular}
\end{table}

At $\lambda=1.0$ (semantic only), CF reconstruction loss is 4.5$\times$ higher than at $\lambda=0.5$, indicating poor collaborative signal preservation. The balanced setting ($\lambda=0.5$) achieves good reconstruction for both signal types.

\subsubsection{Code Uniqueness}

Both \ours{} and semantic-only achieve $>$97\% unique code assignments across items, ensuring distinct identifiers for the generative model.

% ============================================================================
% ANALYSIS
% ============================================================================
\section{Analysis}
\label{sec:analysis}

\subsection{Why Does \ours{} Work?}

\textbf{Complementary Information:} Semantic embeddings capture \emph{what} an item is (content, category, attributes), while CF embeddings capture \emph{who} interacts with it (user preferences, behavioral patterns). These signals are complementary: items similar in content may have different audiences, and vice versa.

\textbf{Hierarchical Encoding:} The residual quantization creates a coarse-to-fine hierarchy. Early levels capture broad categories (informed by both content and behavioral clusters), while later levels capture fine-grained distinctions.

\textbf{Shared Discrete Space:} By encoding both signals into the same discrete ID space, the generative model can implicitly leverage both types of information when predicting next items.

\subsection{Limitations and Future Work}

\begin{itemize}
    \item \textbf{Cold-start items:} Items without interaction history cannot benefit from CF signals. Future work could explore content-based fallback or transfer learning.
    \item \textbf{Computational cost:} Training LightGCN adds preprocessing overhead, though this is one-time.
    \item \textbf{Single dataset:} Evaluation on additional domains (e.g., movies, music) would strengthen claims.
\end{itemize}

% ============================================================================
% CONCLUSION
% ============================================================================
\section{Conclusion}
\label{sec:conclusion}

We presented \ours{}, a novel approach for learning discrete item identifiers that jointly capture semantic content and collaborative filtering signals. Through multi-objective residual quantization, \ours{} creates hierarchical representations suitable for generative recommendation while preserving both information types. Experiments demonstrate substantial improvements over semantic-only baselines (+144\% Recall@10), validating our hypothesis that collaborative signals provide complementary value for item ID learning in generative recommendation systems.

% ============================================================================
% REFERENCES
% ============================================================================
\bibliography{references}
\bibliographystyle{icml2026}

\end{document}
